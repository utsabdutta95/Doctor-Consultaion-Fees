{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset includes details of Doctor Consultaion fees, below are some variables of the datset:\n",
    "- Qualification: Qualification and degrees held by the doctor\n",
    "\n",
    "- Experience: Experience of the doctor in number of years\n",
    "\n",
    "- Rating: Rating given by patients\n",
    "\n",
    "- Profile: Type of the doctor\n",
    "\n",
    "- Miscellaneous_Info: Extra information about the doctor\n",
    "\n",
    "- Place: Area and the city where the doctor is located.\n",
    "\n",
    "TARGET VARIABLE --> Fees: Fees charged by the doctor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM STATEMENT :\n",
    "\n",
    "We have all been in situation where we go to a doctor in emergency and find that the consultation fees are too high. As a data scientist we all should do better. What if you have data that records important details about a doctor and you get to build a model to predict the doctorâ€™s consulting fee.? This is the use case that let's you do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, from the problem statement and the Dataset we can understand that it is a \"Regression problem\". so we will be using some Regression algorithms to make our model and then use GRIDSEARCHCV for hypeparameter tuning and save the predicted model using pkl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION/Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Final_Train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the columns of the dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6 independent variables and 1 target variable, i.e. Fees in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the head of the Dataset to get a general view of the Data we will be working with.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by seeing the data we get a general understanding that some Data Cleaning is needed in the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking The Data Dimension\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull())\n",
    "plt.title('Null values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So many white lines, telling us the presence of many null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get some more information about the Dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above we can see that there are two types of values present in the dataset, i.e ,int64 and object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get a general idea about the dataset by the describe method\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clean and adjust the Experience column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract years of experience\n",
    "df[\"Experience\"] = df[\"Experience\"].str.split()\n",
    "df[\"Experience\"] = df[\"Experience\"].str[0].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clean and adjust the Place column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cities\n",
    "\n",
    "\n",
    "df[\"Place\"].fillna(\"Unknown,Unknown\",inplace=True)\n",
    "df[\"Place\"] = df[\"Place\"].str.split(\",\")\n",
    "df[\"City\"] = df[\"Place\"].str[-1]\n",
    "df[\"Place\"] = df[\"Place\"].str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fill the missing values in Rating column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Ratings into bins\n",
    "df[\"Rating\"].fillna(\"-99%\",inplace=True)\n",
    "df[\"Rating\"] = df[\"Rating\"].str[:-1].astype(\"int\")\n",
    "bins = [-99,0,10,20,30,40,50,60,70,80,90,100]\n",
    "labels = [i for i in range(11)]\n",
    "df[\"Rating\"] = pd.cut(df[\"Rating\"],bins=bins,labels=labels,include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see the value counts of rating column\n",
    "df['Rating'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the Qualification column data as it has many things which needs cleaning before modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant qualification\n",
    "df[\"Qualification\"]=df[\"Qualification\"].str.split(\",\")\n",
    "Qualification ={}\n",
    "for x in df[\"Qualification\"].values:\n",
    "    for each in x:\n",
    "        each = each.strip()\n",
    "        if each in Qualification:\n",
    "            Qualification[each]+=1\n",
    "        else:\n",
    "            Qualification[each]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_qua = sorted(Qualification.items(),key=lambda x:x[1],reverse=True)[:10]\n",
    "final_qua =[]\n",
    "for tup in most_qua:\n",
    "    final_qua.append(tup[0])\n",
    "for title in final_qua:\n",
    "    df[title]=0\n",
    "    \n",
    "for x,y in zip(df[\"Qualification\"].values,np.array([idx for idx in range(len(df))])):\n",
    "    for q in x:\n",
    "        q = q.strip()\n",
    "        if q in final_qua:\n",
    "            df[q][y] = 1\n",
    "df.drop(\"Qualification\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the value counts of Profile column\n",
    "df['Profile'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the value counts of city column\n",
    "df['City'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that there is an column named 'e' which we will deal next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"City\"][3980] = \"Unknown\"\n",
    "df[\"Place\"][3980] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets see again if the column 'e' is removed or not.\n",
    "df['City'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies\n",
    "df = pd.get_dummies(df,columns=[\"City\",\"Profile\"],prefix=[\"City\",\"Profile\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of Cities are less, We can dummify the city names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Miscellaneous_Info'].value_counts ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will drop the 'Miscellaneous_Info' column as I am no NLP expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the column\n",
    "df.drop(\"Miscellaneous_Info\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are dropping the Miscellaneous_Info section as I am No NLP expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets again check the head of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check if we have cleaned the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above we can see that some new columns have come into existence.\n",
    "- There are now five dtypes present i.e category, int32, int64, object, uint8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets again check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that there are no null values present in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets do some EDA over the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets check some correlation from the dataset\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the correlations of features with the target. No correlations are extremely high. So we will take every variables into action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating independent variable and target variable and also encoding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Fees\",axis=1)\n",
    "y = df[\"Fees\"]\n",
    "\n",
    "# Encoding\n",
    "enc = OrdinalEncoder()\n",
    "X = enc.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "# feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score(y_pred,y):\n",
    "    y_pred = np.log(y_pred)\n",
    "    y = np.log(y)\n",
    "    return 1 - ((np.sum((y_pred-y)**2))/len(y))**1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define own scorer\n",
    "scorer = make_scorer(score,greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# support vector machine \n",
    "from sklearn.svm import SVR\n",
    "m = SVR(gamma=\"scale\")\n",
    "m.fit(scaler.transform(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = m.predict(scaler.transform(X_test))\n",
    "score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIDSEARCHCV/HYPERMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tunning\n",
    "parameters = {\"C\":[0.1,1,10],\"kernel\":[\"linear\",\"rbf\",\"poly\"]}\n",
    "reg = GridSearchCV(m,param_grid=parameters,scoring=scorer,n_jobs=-1,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned = reg.predict(scaler.transform(X_test))\n",
    "score(y_pred_tuned,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above we can see that we increased the accuracy score drastically.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Best Model Using PKl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "filename = 'fees_model.pkl'\n",
    "joblib.dump(y_pred_tuned, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
